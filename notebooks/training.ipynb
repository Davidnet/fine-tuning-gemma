{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"mlops-explorations\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_URI = \"gs://mlops-fine-tuning-gemma\"\n",
    "SERVICE_ACCOUNT = \"939282436854-compute@developer.gserviceaccount.com\"\n",
    "VERTEX_AI_MODEL_GARDEN_GEMMA = \"https://storage.googleapis.com/vertex-ai/generative-ai/model-garden/gemma.tar.gz\"\n",
    "# ETL: https://colab.research.google.com/drive/1eBPABHwRo6vyAdUoBJ5ERVS8xRe-wwLv?usp=sharing\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"gemma\")\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240220_0936_RC01\"\n",
    "base_model_path_prefix = MODEL_BUCKET\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"HuggingFaceH4/CodeAlpaca_20K\"\n",
    "instruct_column_in_dataset = \"text\"\n",
    "template = \"gs://mlops-fine-tuning-gemma/templates/code-alpaca.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://mlops-fine-tuning-gemma/temporal/aiplatform-custom-training-2024-04-08-02:50:06.352 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4304458314730700800?project=939282436854\n"
     ]
    }
   ],
   "source": [
    "# The Gemma base model.\n",
    "MODEL_ID = \"gemma-2b\" # [\"gemma-2b\", \"gemma-2b-it\", \"gemma-7b\", \"gemma-7b-it\"]\n",
    "# The accelerator to use.\n",
    "ACCELERATOR_TYPE = \"NVIDIA_L4\"  # [\"NVIDIA_TESLA_V100\", \"NVIDIA_L4\", \"NVIDIA_TESLA_A100\"]\n",
    "base_model_id = os.path.join(base_model_path_prefix, MODEL_ID)\n",
    "\n",
    "# Batch size for finetuning.\n",
    "per_device_train_batch_size = 1  \n",
    "max_steps = 1000  # Runs 10 training steps as a minimal example.\n",
    "# Precision mode for finetuning.\n",
    "finetuning_precision_mode = \"float16\"  # [\"4bit\", \"8bit\", \"float16\"]\n",
    "# Learning rate.\n",
    "learning_rate = 2e-4  # \n",
    "# LoRA parameters.\n",
    "lora_rank = 16  \n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Worker pool spec.\n",
    "# Gemma only supports single-GPU finetuning.\n",
    "accelerator_count = 1\n",
    "machine_type = None\n",
    "if \"7b\" in MODEL_ID:\n",
    "    assert (\n",
    "        ACCELERATOR_TYPE != \"NVIDIA_TESLA_V100\"\n",
    "    ), \"Finetuning Gemma 7B models is not supported on NVIDIA_TESLA_V100. Try a GPU with more VRAM.\"\n",
    "    if ACCELERATOR_TYPE == \"NVIDIA_L4\":\n",
    "        assert (\n",
    "            finetuning_precision_mode == \"4bit\"\n",
    "        ), f\"{finetuning_precision_mode} finetuning of Gemma 7B models is not supported on NVIDIA_L4. Try a GPU with more VRAM or use a different precision mode.\"\n",
    "\n",
    "if ACCELERATOR_TYPE == \"NVIDIA_TESLA_V100\":\n",
    "    machine_type = \"n1-standard-8\"\n",
    "elif ACCELERATOR_TYPE == \"NVIDIA_L4\":\n",
    "    machine_type = \"g2-standard-12\"\n",
    "elif ACCELERATOR_TYPE == \"NVIDIA_TESLA_A100\":\n",
    "    machine_type = \"a2-highgpu-1g\"\n",
    "\n",
    "assert (\n",
    "    machine_type\n",
    "), f\"Cannot automatically determine machine type from {ACCELERATOR_TYPE}.\"\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Setup training job.\n",
    "job_name = get_job_name_with_datetime(\"gemma-lora-train\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "\n",
    "# Create a GCS folder to store the LORA adapter.\n",
    "lora_adapter_dir = get_job_name_with_datetime(\"gemma-lora-adapter\")\n",
    "lora_output_dir = os.path.join(STAGING_BUCKET, lora_adapter_dir)\n",
    "\n",
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# finetuned LORA adapter.\n",
    "merged_model_dir = get_job_name_with_datetime(\"gemma-merged-model\")\n",
    "merged_model_output_dir = os.path.join(STAGING_BUCKET, merged_model_dir)\n",
    "\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=instruct-lora\",\n",
    "        f\"--pretrained_model_id={base_model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--instruct_column_in_dataset={instruct_column_in_dataset}\",\n",
    "        f\"--output_dir={lora_output_dir}\",\n",
    "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
    "        f\"--per_device_train_batch_size={per_device_train_batch_size}\",\n",
    "        f\"--lora_rank={lora_rank}\",\n",
    "        f\"--lora_alpha={lora_alpha}\",\n",
    "        f\"--lora_dropout={lora_dropout}\",\n",
    "        f\"--max_steps={max_steps}\",\n",
    "        \"--max_seq_length=512\",\n",
    "        f\"--learning_rate={learning_rate}\",\n",
    "        f\"--precision_mode={finetuning_precision_mode}\",\n",
    "        f\"--template={template}\"\n",
    "    ],\n",
    "    environment_variables={\"WANDB_DISABLED\": True},\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapter was saved in: \", lora_output_dir)\n",
    "print(\"Trained and merged models were saved in: \", merged_model_output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
